# run_graph.py

import functools
import sys
import os

# --- åŠ¨æ€æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonæœç´¢è·¯å¾„ ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from langgraph.graph import StateGraph, END

# --- å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å— ---
from rag_system.graph_state import GraphState
from rag_system.planner.planner import Planner, plan_node
from rag_system.executor.executor import Executor, execute_node
from rag_system.reflector.reflector import Reflector, reflect_node
from rag_system.decider.decider import should_continue

from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from rag_system.config import settings

def generate_final_answer_node(state: GraphState) -> dict:
    """
    ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹
    """
    print("--- [èŠ‚ç‚¹: Final Answer Generator] ---")
    llm = ChatOllama(model=settings.LOCAL_LLM_MODEL_NAME, temperature=0.1)
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©ç†ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„åŸå§‹é—®é¢˜å’Œç³»ç»Ÿçš„æ‰§è¡Œå†å²ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€æ¸…æ™°ä¸”å‹å¥½çš„æœ€ç»ˆç­”æ¡ˆã€‚\n\n"
        "ã€ç”¨æˆ·åŸå§‹é—®é¢˜ã€‘:\n{initial_query}\n\n"
        "ã€ç³»ç»Ÿæ‰§è¡Œå†å²æ‘˜è¦ã€‘:\n{history}\n\n"
        "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ:"
    )
    chain = prompt | llm | StrOutputParser()
    history_summary = "\n".join([str(item) for item in state['history']])
    final_answer = chain.invoke({
        "initial_query": state['initial_query'],
        "history": history_summary
    })
    return {"final_answer": final_answer}

def build_agent_graph() -> StateGraph:
    """
    æ„å»ºå¹¶è¿”å›ä¸€ä¸ªç¼–è¯‘å¥½çš„LangGraphæ™ºèƒ½ä»£ç†ã€‚
    """
    planner = Planner()
    executor = Executor()
    reflector = Reflector()

    bound_plan_node = functools.partial(plan_node, planner_instance=planner)
    bound_execute_node = functools.partial(execute_node, executor_instance=executor)
    bound_reflect_node = functools.partial(reflect_node, reflector_instance=reflector)

    workflow = StateGraph(GraphState)

    workflow.add_node("planner", bound_plan_node)
    workflow.add_node("executor", bound_execute_node)
    workflow.add_node("reflector", bound_reflect_node)
    workflow.add_node("final_answer_generator", generate_final_answer_node)

    workflow.set_entry_point("planner")

    workflow.add_edge("planner", "executor")
    workflow.add_edge("executor", "reflector")
    workflow.add_edge("final_answer_generator", END)

    workflow.add_conditional_edges(
        "reflector",
        should_continue,
        {
            "replan": "planner",
            "continue_execute": "executor",
            END: "final_answer_generator"
        }
    )

    app = workflow.compile()
    print("âœ… Agent graph compiled successfully!")
    return app

if __name__ == "__main__":
    agent_app = build_agent_graph()
    query = "TFNè†œçš„ç»“æ„å’Œæ€§èƒ½æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿè¯·ç»“åˆè¿‘ä¸‰å¹´çš„è®ºæ–‡è¿›è¡Œæ€»ç»“ã€‚"
    initial_state = {
        "initial_query": query,
        "plan": None,
        "history": [],
        "error_count": 0,
        "final_answer": None,
    }

    print(f"\nğŸš€ Starting Agent with query: '{query}'\n" + "="*50)

    final_state = None
    # [ä¿®æ”¹] æˆ‘ä»¬åªä½¿ç”¨streamæ¥è¿è¡Œï¼Œå¹¶åœ¨å¾ªç¯ä¸­ä¿å­˜æœ€åçš„çŠ¶æ€
    for step_output in agent_app.stream(initial_state, stream_mode="values"):
        node_name = list(step_output.keys())[0]
        node_output = list(step_output.values())[0]
        print(f"\n--- âœ… Executed Node: {node_name} ---")
        print(f"Node Output:\n{node_output}")
        print("-" * 50)
        # ä¿å­˜æœ€æ–°çš„å®Œæ•´çŠ¶æ€
        final_state = step_output

    print("\nğŸ Agent run has finished.")

    if final_state and final_state.get('final_answer'):
        print("\n\nFINAL ANSWER:")
        print(final_state['final_answer'])
    else:
        print("\n\nAgent finished without a final answer. Dumping final state:")
        print(final_state)

