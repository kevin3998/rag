# app.py (æœ€ç»ˆä¿®å¤ç‰ˆ)

import streamlit as st
import functools
import sys
import os
from typing import List

# --- è·¯å¾„è®¾ç½® ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# --- LangChain åŠé¡¹ç›®æ¨¡å—å¯¼å…¥ ---
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser

# å¯¼å…¥æ‚¨çš„æ ¸å¿ƒæ¨¡å—
from rag_system.graph_state import GraphState, Step
from rag_system.config import settings
from rag_system.planner.planner import Planner, plan_node
from rag_system.executor.executor import Executor, execute_node
from rag_system.reflector.reflector import Reflector, reflect_node
from rag_system.decider.decider import should_continue

# å¯¼å…¥æ‰€æœ‰éœ€è¦ç”¨åˆ°çš„å·¥å…·
from rag_system.agent.tools.paper_finder_tool import paper_finder_tool
from rag_system.agent.tools.semantic_search import semantic_search_tool
from rag_system.agent.tools.prediction_tool import prediction_tool

# --- åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ ---
try:
    print("--- æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰Agentç»„ä»¶ ---")

    tools = [paper_finder_tool, semantic_search_tool, prediction_tool]
    general_llm = ChatOllama(model=settings.PREDICTION_MODEL_NAME, temperature=0)

    # [æœ€ç»ˆä¿®å¤] æ ¹æ®æ‚¨çš„ç¡®è®¤ï¼Œç²¾ç¡®åœ°ä¸ºæ¯ä¸ªç»„ä»¶æä¾›å…¶æ‰€éœ€çš„å‚æ•°
    planner_instance = Planner(tools=tools)  # Planner éœ€è¦ tools
    executor_instance = Executor()  # Executor ä¸éœ€è¦å‚æ•°
    reflector_instance = Reflector()  # Reflector ä¸éœ€è¦å‚æ•°

    print("âœ… æ‰€æœ‰Agentç»„ä»¶åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
    st.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥ï¼šæ— æ³•åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ã€‚é”™è¯¯ä¿¡æ¯: {e}")
    sys.exit(1)


# --- è·¯ç”±å’ŒèŠ‚ç‚¹å‡½æ•°å®šä¹‰ (è¿™éƒ¨åˆ†é€»è¾‘å·²æ­£ç¡®ï¼Œæ— éœ€æ”¹åŠ¨) ---

class RouteQuery(BaseModel):
    datasource: str = Field(enum=["membrane_query", "general_chat", "out_of_domain_query"])


def get_router(llm):
    parser = JsonOutputParser(pydantic_object=RouteQuery)
    prompt = PromptTemplate(
        template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
        ä½ æ˜¯ä¸€ä¸ªé«˜çº§é—®é¢˜åˆ†ç±»ä¸“å®¶ã€‚ä½ çš„æ ¸å¿ƒçŸ¥è¯†é¢†åŸŸä¸¥æ ¼é™å®šåœ¨â€œè†œç§‘å­¦ä¸æŠ€æœ¯â€ã€‚è¯·æ ¹æ®ç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œå°†å…¶åˆ†ç±»åˆ°ä»¥ä¸‹ä¸‰ä¸ªç±»åˆ«ä¹‹ä¸€ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚
        1. 'membrane_query': ä¸è†œç§‘å­¦ç›´æ¥ç›¸å…³çš„é—®é¢˜ã€‚
        2. 'general_chat': æ—¥å¸¸é—®å€™ã€å…³äºä½ èº«ä»½çš„é—²èŠã€‚
        3. 'out_of_domain_query': å…¶ä»–é¢†åŸŸçš„ä¸“ä¸šé—®é¢˜ã€‚
        {format_instructions}
        ç”¨æˆ·æœ€æ–°çš„é—®é¢˜: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
        input_variables=["question"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    json_llm = llm.bind(format="json")
    return prompt | json_llm | parser


def classify_question_node(state: GraphState):
    question = state['initial_query']
    router_chain = get_router(general_llm)
    result = router_chain.invoke({"question": question})
    decision = result.get("datasource")
    return {"route_decision": decision}


def decide_next_step(state: GraphState) -> str:
    return state.get('route_decision') or "planner"


def general_chat_node(state: GraphState):
    question = state['initial_query']
    persona_prompt = f"ä½ æ˜¯ 'RAG-Intelligent-Researcher' AIåŠ©æ‰‹ã€‚è¯·å‹å¥½ã€ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\nç”¨æˆ·é—®é¢˜: {question}"
    response = general_llm.invoke(persona_prompt)
    return {"generation": response.content}


def decline_answer_node(state: GraphState):
    generation = "éå¸¸æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†èŒƒå›´ä¸»è¦é›†ä¸­åœ¨â€œè†œç§‘å­¦ä¸æŠ€æœ¯â€é¢†åŸŸã€‚æ‚¨æå‡ºçš„é—®é¢˜è¶…å‡ºäº†æˆ‘çš„ä¸“ä¸šèŒƒç•´ï¼Œä¸ºäº†é¿å…æä¾›ä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œæˆ‘æ— æ³•ç»™å‡ºæœ‰æ•ˆçš„å›ç­”ã€‚"
    return {"generation": generation}


def generate_final_answer_node(state: GraphState) -> dict:
    print("--- [èŠ‚ç‚¹: Generate Final Answer] ---")

    # ä¼˜å…ˆå¤„ç†æ¥è‡ªé—²èŠæˆ–æ‹’ç»èŠ‚ç‚¹çš„ç›´æ¥ç”Ÿæˆå†…å®¹
    if generation := state.get("generation"):
        return {"final_answer": generation}

    print("    - [æ¥æº]: RAG Pipeline")

    # è·å–æœ€åä¸€ä¸ªæ‰§è¡Œæ­¥éª¤
    last_step = state['history'][-1]

    # æ£€æŸ¥æœ€åä¸€ä¸ªæ­¥éª¤æ˜¯å¦æ˜¯æˆåŠŸçš„å·¥å…·è°ƒç”¨ï¼Œå¹¶ä¸”è¿”å›äº†ç»“æœ
    if isinstance(last_step, Step) and last_step.is_success and last_step.result:
        # å¦‚æœæœ€åä¸€ä¸ªå·¥å…·æ˜¯semantic_searchæˆ–predictionï¼Œå®ƒä»¬çš„ç»“æœæœ¬èº«å°±æ˜¯ä¸€ä»½å®Œæ•´çš„æŠ¥å‘Š
        if last_step.tool_name in ["semantic_search_tool", "prediction_tool"]:
            print("    - [ç­–ç•¥]: ç›´æ¥ä½¿ç”¨æœ€åä¸€ä¸ªå·¥å…·çš„è¾“å‡ºä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚")
            return {"final_answer": last_step.result}

    # å¦‚æœä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼Œåˆ™æ‰§è¡ŒåŸæœ‰çš„â€œå…œåº•â€æ€»ç»“é€»è¾‘
    print("    - [ç­–ç•¥]: å¯¹æ‰€æœ‰å†å²ç»“æœè¿›è¡Œæœ€ç»ˆæ€»ç»“ã€‚")
    final_context_list = []
    for item in state['history']:
        if isinstance(item, Step) and item.result is not None:
            final_context_list.append(str(item.result))
    final_context = "\n\n---\n\n".join(final_context_list)

    if not final_context:
        final_context = "æœªèƒ½ä»æ‰§è¡Œå†å²ä¸­æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç»“æœã€‚"

    prompt = PromptTemplate.from_template(
        "# è§’è‰²: ä¸¥è°¨çš„ç§‘ç ”åŠ©ç†...\nã€åŸå§‹é—®é¢˜ã€‘:{initial_query}\nã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘:{final_context}\nä½ çš„æœ€ç»ˆç­”æ¡ˆ:")
    final_chain = prompt | general_llm
    response = final_chain.invoke({"initial_query": state['initial_query'], "final_context": final_context})
    return {"final_answer": response.content}

# --- [æ ¸å¿ƒä¿®å¤] æ„å»ºæœ€ç»ˆçš„Graph ---
def build_agent_graph():
    workflow = StateGraph(GraphState)

    plan_node_partial = functools.partial(plan_node, planner_instance=planner_instance)
    execute_node_partial = functools.partial(execute_node, executor_instance=executor_instance)
    reflect_node_partial = functools.partial(reflect_node, reflector_instance=reflector_instance)

    workflow.add_node("classifier", classify_question_node)
    workflow.add_node("general_chat", general_chat_node)
    workflow.add_node("decline_node", decline_answer_node)
    workflow.add_node("planner", plan_node_partial)
    workflow.add_node("executor", execute_node_partial)
    workflow.add_node("reflector", reflect_node_partial)
    workflow.add_node("final_answer_generator", generate_final_answer_node)

    workflow.set_entry_point("classifier")

    workflow.add_conditional_edges("classifier", decide_next_step, {
        "membrane_query": "planner",
        "general_chat": "general_chat",
        "out_of_domain_query": "decline_node"
    })

    workflow.add_edge("general_chat", "final_answer_generator")
    workflow.add_edge("decline_node", "final_answer_generator")
    workflow.add_edge("planner", "executor")
    workflow.add_edge("executor", "reflector")

    # [æ ¸å¿ƒä¿®å¤] å°†è¿™é‡Œçš„é”®æ”¹å› 'finish'ï¼Œä»¥åŒ¹é…æ‚¨çš„deciderçš„å®é™…è¾“å‡º
    workflow.add_conditional_edges(
        "reflector",
        should_continue,
        {
            "continue_execute": "executor",
            "replan": "planner",
            "finish": "final_answer_generator",  # <-- å·²æ”¹å› 'finish'
        }
    )

    workflow.add_edge("final_answer_generator", END)

    print("âœ… LangGraphå·¥ä½œæµç¼–è¯‘å®Œæˆã€‚")
    return workflow.compile()


# --- Streamlit UI (ä¿æŒä¸å˜) ---
st.title("ğŸ¤– RAG æ™ºèƒ½ç§‘ç ”åŠ©æ‰‹")
st.markdown("ä¸€ä¸ªå…·å¤‡è§„åˆ’ã€æ‰§è¡Œã€åæ€å’Œè®°å¿†èƒ½åŠ›çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚ç”±æ‚¨å¾®è°ƒçš„Qwen3é©±åŠ¨ã€‚")

if "agent_app" not in st.session_state:
    st.session_state.agent_app = build_agent_graph()

if "messages" not in st.session_state:
    st.session_state.messages = []

if not st.session_state.messages:
    with st.chat_message("ai"):
        st.markdown("æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIç§‘ç ”åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ")
else:
    for message in st.session_state.messages:
        with st.chat_message(message.type):
            st.markdown(message.content)

if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    initial_state = {
        "initial_query": prompt,
        "history": [],
        "chat_history": st.session_state.messages[:-1]
    }

    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  **æ€è€ƒä¸­...**"):
            try:
                final_state = st.session_state.agent_app.invoke(initial_state)
                final_answer = final_state.get("final_answer", "æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œæ— æ³•ç”Ÿæˆå›ç­”ã€‚")
            except Exception as e:
                final_answer = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}"
                st.error(final_answer)

        st.markdown(final_answer)
        st.session_state.messages.append(AIMessage(content=final_answer))