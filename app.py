# app.py

import streamlit as st
import functools
import sys
import os
from typing import List

# --- è·¯å¾„è®¾ç½® ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# --- LangChain æ ¸å¿ƒå¯¼å…¥ ---
from langgraph.graph import StateGraph, END
from langgraph.pregel import Pregel
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

# --- å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å— ---
from rag_system.graph_state import GraphState, Step
from rag_system.planner.planner import Planner, plan_node
from rag_system.executor.executor import Executor, execute_node
from rag_system.reflector.reflector import Reflector, reflect_node
from rag_system.decider.decider import should_continue
from rag_system.config import settings

# --- æœ€ç»ˆç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹ ---
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate


# [ä¿®æ”¹] æˆ‘ä»¬ä¸å†éœ€è¦ StrOutputParser
# from langchain_core.output_parsers import StrOutputParser

def generate_final_answer_node(state: GraphState) -> dict:
    """
    ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹
    """
    print("--- [èŠ‚ç‚¹: Final Answer Generator] ---")
    llm = ChatOllama(model=settings.LOCAL_LLM_MODEL_NAME, temperature=0.1)

    prompt = PromptTemplate.from_template(
        """# è§’è‰²
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç§‘ç ”åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘å’Œç”¨æˆ·çš„ã€åŸå§‹é—®é¢˜ã€‘ï¼Œç”Ÿæˆä¸€ä¸ªæ¸…æ™°ã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚

        # ä»»åŠ¡
        1.  **åˆ†æé—®é¢˜ç±»å‹**: åˆ¤æ–­ç”¨æˆ·çš„ã€åŸå§‹é—®é¢˜ã€‘æ˜¯è¦æ±‚ä¸€ä¸ªã€åˆ—è¡¨ã€‘ï¼Œè¿˜æ˜¯è¦æ±‚ä¸€ä¸ªã€åˆ†ææ€»ç»“ã€‘ã€‚
        2.  **å¿ å®å‘ˆç°**:
            - å¦‚æœé—®é¢˜è¦æ±‚ä¸€ä¸ªåˆ—è¡¨ï¼ˆä¾‹å¦‚ï¼Œâ€œæ‰¾å‡ºæ‰€æœ‰...çš„è®ºæ–‡â€ï¼‰ï¼Œä½ çš„å›ç­”åº”è¯¥ç›´æ¥ã€å®Œæ•´åœ°åˆ—å‡ºã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘ä¸­çš„æ‰€æœ‰é¡¹ç›®ï¼Œå¹¶å¯ä»¥é™„ä¸Šä¸€å¥æ€»ç»“ï¼Œä¾‹å¦‚â€œæ ¹æ®æ‚¨çš„æ¡ä»¶ï¼Œå…±æ‰¾åˆ° N ç¯‡è®ºæ–‡å¦‚ä¸‹ï¼šâ€ã€‚
            - å¦‚æœé—®é¢˜è¦æ±‚åˆ†ææ€»ç»“ï¼Œè¯·åŸºäºã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘è¿›è¡Œæ·±å…¥åˆ†æã€‚
        3.  **ä¸è¦ç¼–é€ **: ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä¸¥æ ¼åŸºäºã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘ã€‚

        ---
        ã€åŸå§‹é—®é¢˜ã€‘:
        {initial_query}
        ---
        ã€æœ€ç»ˆæ£€ç´¢ç»“æœã€‘:
        {final_context}
        ---
        ä½ çš„æœ€ç»ˆç­”æ¡ˆ:
        """
    )

    # ================== [ å…³ é”® ä¿® å¤ ] ==================
    # 1. æˆ‘ä»¬çš„é“¾ç°åœ¨åªåŒ…å«Promptå’ŒLLM
    chain = prompt | llm

    # 2. æˆ‘ä»¬åªå°†æœ€åä¸€æ­¥æˆåŠŸçš„ç»“æœä½œä¸ºæœ€ç»ˆä¸Šä¸‹æ–‡
    last_successful_result = next(
        (step.result for step in reversed(state['history']) if isinstance(step, Step) and step.is_success),
        "æœªèƒ½ä»æ‰§è¡Œå†å²ä¸­æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆç»“æœã€‚"
    )

    # 3. è°ƒç”¨é“¾ï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ªAIMessageå¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
    ai_message_result = chain.invoke({
        "initial_query": state['initial_query'],
        "final_context": str(last_successful_result)
    })

    # 4. æˆ‘ä»¬æ‰‹åŠ¨ä»AIMessageå¯¹è±¡ä¸­æå–å†…å®¹å­—ç¬¦ä¸²
    final_answer = ai_message_result.content
    # =====================================================

    return {"final_answer": final_answer}


# ... (build_agent_graph å’Œ Streamlit çš„ä¸»é€»è¾‘ä¿æŒä¸å˜) ...
@st.cache_resource
def build_agent_graph() -> Pregel:
    print("--- Initializing Agent Graph for the first time ---")
    executor = Executor()
    tools = list(executor.tools.values())
    planner = Planner(tools=tools)
    reflector = Reflector()
    bound_plan_node = functools.partial(plan_node, planner_instance=planner)
    bound_execute_node = functools.partial(execute_node, executor_instance=executor)
    bound_reflect_node = functools.partial(reflect_node, reflector_instance=reflector)
    workflow = StateGraph(GraphState)
    workflow.add_node("planner", bound_plan_node)
    workflow.add_node("executor", bound_execute_node)
    workflow.add_node("reflector", bound_reflect_node)
    workflow.add_node("final_answer_generator", generate_final_answer_node)
    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "executor")
    workflow.add_edge("executor", "reflector")
    workflow.add_edge("final_answer_generator", END)
    workflow.add_conditional_edges(
        "reflector", should_continue,
        {"replan": "planner", "continue_execute": "executor", END: "final_answer_generator"}
    )
    app = workflow.compile()
    print("âœ… Agent graph compiled successfully!")
    return app


st.set_page_config(page_title="ğŸ”¬ LangGraph RAG Agent", layout="wide")
st.title("ğŸ”¬ LangGraph RAG Agent")
st.markdown("ä¸€ä¸ªå…·å¤‡è§„åˆ’ã€æ‰§è¡Œã€åæ€å’Œè®°å¿†èƒ½åŠ›çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚ç”±æ‚¨å¾®è°ƒçš„Qwen3é©±åŠ¨ã€‚")
with st.sidebar:
    st.header("å…³äº")
    st.markdown("""
    è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†ä¸€ä¸ªåŸºäºLangGraphæ„å»ºçš„é«˜çº§RAG Agentã€‚å®ƒèƒ½å¤Ÿï¼š
    - **ç†è§£å¯¹è¯å†å²**
    - **åŠ¨æ€è§„åˆ’**ä»»åŠ¡æ­¥éª¤
    - **è°ƒç”¨å·¥å…·**æ£€ç´¢çŸ¥è¯†
    - **åæ€ç»“æœ**å¹¶**è‡ªæˆ‘çº æ­£**
    """)
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()
if "messages" not in st.session_state:
    st.session_state.messages: List[BaseMessage] = []
agent_app = build_agent_graph()
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.markdown(message.content)
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)
    initial_state = {
        "initial_query": prompt,
        "plan": None,
        "history": [],
        "error_count": 0,
        "final_answer": None,
        "chat_history": st.session_state.messages[:-1]
    }
    with st.chat_message("assistant"):
        thinking_container = st.container()
        final_state = None
        for step_output in agent_app.stream(initial_state, stream_mode="values"):
            node_name = list(step_output.keys())[0]
            thinking_container.write(f"ğŸ§  **æ€è€ƒä¸­... [æ‰§è¡ŒèŠ‚ç‚¹: {node_name}]**")
            final_state = step_output
        final_answer = final_state.get("final_answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¾—å‡ºç»“è®ºã€‚")
        st.markdown(final_answer)
    st.session_state.messages.append(AIMessage(content=final_answer))
