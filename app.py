# app.py

import streamlit as st
import functools
import sys
import os
from typing import List

# --- è·¯å¾„è®¾ç½® ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# --- LangGraph å’Œ LangChain æ ¸å¿ƒå¯¼å…¥ ---
from langgraph.graph import StateGraph, END
from langgraph.pregel import Pregel
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage

# --- å¯¼å…¥æˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å—å’ŒçŠ¶æ€å®šä¹‰ ---
from rag_system.graph_state import GraphState
from rag_system.planner.planner import Planner, plan_node
from rag_system.executor.executor import Executor, execute_node
from rag_system.reflector.reflector import Reflector, reflect_node
from rag_system.decider.decider import should_continue
from rag_system.config import settings

# --- æœ€ç»ˆç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹ ---
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


def generate_final_answer_node(state: GraphState) -> dict:
    # ... (æ­¤å‡½æ•°ä¿æŒä¸å˜) ...
    print("--- [èŠ‚ç‚¹: Final Answer Generator] ---")
    llm = ChatOllama(model=settings.LOCAL_LLM_MODEL_NAME, temperature=0.1)
    prompt = PromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªç§‘ç ”åŠ©ç†ï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·çš„åŸå§‹é—®é¢˜å’Œç³»ç»Ÿçš„å®Œæ•´æ‰§è¡Œå†å²ï¼ˆåŒ…æ‹¬ä¹‹å‰çš„å¯¹è¯ï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€æ¸…æ™°ä¸”å‹å¥½çš„æœ€ç»ˆç­”æ¡ˆã€‚\n\n"
        "ã€å†å²å¯¹è¯ã€‘:\n{chat_history}\n\n"
        "ã€å½“å‰ä»»åŠ¡æ‰§è¡Œå†å²ã€‘:\n{history}\n\n"
        "ã€ç”¨æˆ·å½“å‰é—®é¢˜ã€‘:\n{initial_query}\n\n"
        "è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼ˆä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–XMLæ ‡ç­¾ï¼‰:"
    )
    chain = prompt | llm | StrOutputParser()
    chat_history_str = "\n".join([f"{msg.type}: {msg.content}" for msg in state['chat_history']])
    history_summary = "\n".join([str(item) for item in state['history']])
    final_answer = chain.invoke({
        "initial_query": state['initial_query'],
        "history": history_summary,
        "chat_history": chat_history_str
    })
    return {"final_answer": final_answer}


# --- å›¾çš„ç»„è£… ---
@st.cache_resource
def build_agent_graph() -> Pregel:
    """
    æ„å»ºå¹¶è¿”å›ä¸€ä¸ªç¼–è¯‘å¥½çš„ã€å¯ç¼“å­˜çš„LangGraphæ™ºèƒ½ä»£ç†ã€‚
    """
    print("--- Initializing Agent Graph for the first time ---")

    # 1. å…ˆå®ä¾‹åŒ–éœ€è¦å…±äº«å·¥å…·çš„ç»„ä»¶
    executor = Executor()
    tools = list(executor.tools.values())

    # 2. [ä¿®æ”¹] å°†å·¥å…·åˆ—è¡¨ä¼ é€’ç»™Planner
    planner = Planner(tools=tools)
    reflector = Reflector()

    # 3. ç»‘å®šèŠ‚ç‚¹
    bound_plan_node = functools.partial(plan_node, planner_instance=planner)
    bound_execute_node = functools.partial(execute_node, executor_instance=executor)
    bound_reflect_node = functools.partial(reflect_node, reflector_instance=reflector)

    # 4. å®šä¹‰å·¥ä½œæµ
    workflow = StateGraph(GraphState)
    workflow.add_node("planner", bound_plan_node)
    workflow.add_node("executor", bound_execute_node)
    workflow.add_node("reflector", bound_reflect_node)
    workflow.add_node("final_answer_generator", generate_final_answer_node)
    workflow.set_entry_point("planner")
    workflow.add_edge("planner", "executor")
    workflow.add_edge("executor", "reflector")
    workflow.add_edge("final_answer_generator", END)
    workflow.add_conditional_edges(
        "reflector", should_continue,
        {"replan": "planner", "continue_execute": "executor", END: "final_answer_generator"}
    )

    app = workflow.compile()
    print("âœ… Agent graph compiled successfully!")
    return app


# --- Streamlit åº”ç”¨ä¸»é€»è¾‘ (ä¿æŒä¸å˜) ---
st.set_page_config(page_title="ğŸ”¬ LangGraph RAG Agent", layout="wide")
st.title("ğŸ”¬ LangGraph RAG Agent")
st.markdown("ä¸€ä¸ªå…·å¤‡è§„åˆ’ã€æ‰§è¡Œã€åæ€å’Œè®°å¿†èƒ½åŠ›çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ã€‚ç”±æ‚¨å¾®è°ƒçš„Qwen3é©±åŠ¨ã€‚")
with st.sidebar:
    st.header("å…³äº")
    st.markdown("""
    è¿™ä¸ªåº”ç”¨å±•ç¤ºäº†ä¸€ä¸ªåŸºäºLangGraphæ„å»ºçš„é«˜çº§RAG Agentã€‚å®ƒèƒ½å¤Ÿï¼š
    - **ç†è§£å¯¹è¯å†å²**
    - **åŠ¨æ€è§„åˆ’**ä»»åŠ¡æ­¥éª¤
    - **è°ƒç”¨å·¥å…·**æ£€ç´¢çŸ¥è¯†
    - **åæ€ç»“æœ**å¹¶**è‡ªæˆ‘çº æ­£**
    """)
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()
if "messages" not in st.session_state:
    st.session_state.messages: List[BaseMessage] = []
agent_app = build_agent_graph()
for message in st.session_state.messages:
    with st.chat_message(message.type):
        st.markdown(message.content)
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)
    initial_state = {
        "initial_query": prompt,
        "plan": None,
        "history": [],
        "error_count": 0,
        "final_answer": None,
        "chat_history": st.session_state.messages[:-1]
    }
    with st.chat_message("assistant"):
        thinking_container = st.container()
        final_state = None
        for step_output in agent_app.stream(initial_state, stream_mode="values"):
            node_name = list(step_output.keys())[0]
            thinking_container.write(f"ğŸ§  **æ€è€ƒä¸­... [æ‰§è¡ŒèŠ‚ç‚¹: {node_name}]**")
            final_state = step_output
        final_answer = final_state.get("final_answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å¾—å‡ºç»“è®ºã€‚")
        st.markdown(final_answer)
    st.session_state.messages.append(AIMessage(content=final_answer))
