# rag_system/planner/planner.py (ÁªìÊûÑÂåñ‰øÆÂ§çÁâà)

import json
import re
from typing import List
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.messages import BaseMessage
from langchain_core.tools import BaseTool

from rag_system.graph_state import GraphState, Plan, Step, Reflection
from rag_system.config import settings
from rag_system.planner.prompt import PROMPT_TEMPLATE


def _format_tools_description(tools: List[BaseTool]) -> str:
    """Ê†ºÂºèÂåñÂ∑•ÂÖ∑ÂàóË°®Ôºå‰æø‰∫éPlannerËØÜÂà´„ÄÇ"""
    descriptions = []
    for tool in tools:
        schema = tool.args_schema.schema()
        props = schema.get('properties', {})
        required_params = schema.get('required', [])

        desc = (
            f"Â∑•ÂÖ∑ÂêçÁß∞: `{tool.name}`\n"
            f"  - ÊèèËø∞: {tool.description}\n"
            f"  - ÂèÇÊï∞:\n"
        )
        for param_name, param_info in props.items():
            is_required = " (ÂøÖÈúÄ)" if param_name in required_params else " (ÂèØÈÄâ)"
            param_type = param_info.get('type', 'N/A')
            param_desc = param_info.get('description', '')
            desc += f"    - `{param_name}`{is_required}: {param_desc} (Á±ªÂûã: {param_type})\n"
        descriptions.append(desc)
    return "\n".join(descriptions)


class Planner:
    def __init__(self, tools: List[BaseTool]):
        self.llm = ChatOllama(model=settings.LOCAL_LLM_MODEL_NAME, temperature=0.0)
        self.output_parser = PydanticOutputParser(pydantic_object=Plan)

        # üöÄ [ÂÖ≥ÈîÆ‰øÆÂ§ç] Â¢ûÂä† query Âíå context ‰∏§‰∏™ËæìÂÖ•ÂèòÈáè
        self.prompt_template = PromptTemplate(
            template=PROMPT_TEMPLATE,
            input_variables=[
                "user_goal", "tools_description",
                "history_str", "chat_history_str",
                "query", "context"  # Êñ∞Â¢û
            ],
        )
        self.tools_description = _format_tools_description(tools)
        print("‚úÖ Planner initialized with tool-aware prompt.")

    def _extract_json_from_response(self, text: str) -> str:
        match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if match:
            return match.group(1)
        start_index = text.find('{')
        end_index = text.rfind('}')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            return text[start_index:end_index + 1]
        raise ValueError("Response does not contain a valid JSON object.")

    def _format_agent_history(self, history: list) -> str:
        if not history:
            return "[]"
        simplified_history = []
        for item in history:
            if isinstance(item, Step):
                simplified_history.append({
                    "step_id": item.step_id,
                    "tool_name": item.tool_name,
                    "result": str(item.result)[:200] + '...',
                    "is_success": item.is_success
                })
            elif isinstance(item, Reflection):
                simplified_history.append({
                    "critique": item.critique,
                    "suggestion": item.suggestion
                })
        return json.dumps(simplified_history, indent=2, ensure_ascii=False)

    def _format_chat_history(self, chat_history: List[BaseMessage]) -> str:
        if not chat_history:
            return "[]"
        return json.dumps([
            {"role": msg.type, "content": msg.content}
            for msg in chat_history
        ], indent=2, ensure_ascii=False)

    def generate_plan(self, user_query: str, history: list, chat_history: List[BaseMessage]) -> Plan:
        print("ü§î Planner starting to generate a plan (with tool descriptions)...")

        agent_history_str = self._format_agent_history(history)
        chat_history_str = self._format_chat_history(chat_history)

        # üöÄ [ÂÖ≥ÈîÆ‰øÆÂ§ç] ËøôÈáå‰º†ÂÖ• query Âíå contextÔºàÂÖàÁÆÄÂåñ contextÔºâ
        prompt_value = self.prompt_template.invoke({
            "user_goal": user_query,
            "tools_description": self.tools_description,
            "history_str": agent_history_str,
            "chat_history_str": chat_history_str,
            "query": user_query,
            "context": agent_history_str  # ÂèØÊâ©Â±ï‰∏∫ÂÖ∂‰ªñ‰∏ä‰∏ãÊñá
        })

        raw_output = self.llm.invoke(prompt_value).content
        json_string = self._extract_json_from_response(raw_output)
        plan = self.output_parser.parse(json_string)

        print(f"‚úÖ Planner generated a new plan with {len(plan.steps)} steps.")
        return plan


def plan_node(state: GraphState, planner_instance: Planner) -> dict:
    print("--- [ËäÇÁÇπ: Planner] ---")
    try:
        plan_result = planner_instance.generate_plan(
            user_query=state['initial_query'],
            history=state['history'],
            chat_history=state['chat_history']
        )
        return {
            "plan": plan_result,
            "history": state['history'] + [f"Log: Successfully generated a plan for '{plan_result.goal}'."]
        }
    except Exception as e:
        print(f"‚ùå Planner failed to generate a valid plan: {e}")
        error_count = state.get('error_count', 0) + 1
        return {
            "plan": None,
            "history": state['history'] + [f"Error: Planner failed. Reason: {e}"],
            "error_count": error_count
        }
