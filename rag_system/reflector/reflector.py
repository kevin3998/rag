# rag_system/reflector/reflector.py

import json
import re
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser

from rag_system.config import settings
from rag_system.state import AgentState, Reflection  # ä»stateå¯¼å…¥Reflection
from rag_system.reflector.prompt import PROMPT_TEMPLATE
from rag_system.state import ReflectionOutput  # å‡è®¾æ‚¨æœ‰ä¸€ä¸ªschemaæ–‡ä»¶


class Reflector:
    def __init__(self):
        self.llm = ChatOllama(model=settings.LOCAL_LLM_MODEL_NAME, temperature=0.1)
        # æ³¨æ„ï¼šè¿™é‡Œçš„pydantic_objectåº”è¯¥æ˜¯LLMç›´æ¥è¾“å‡ºçš„ç»“æ„ï¼Œè€Œä¸æ˜¯æœ€ç»ˆçš„Reflectionå¯¹è±¡
        self.output_parser = PydanticOutputParser(pydantic_object=ReflectionOutput)
        self.prompt_template = PromptTemplate(
            template=PROMPT_TEMPLATE,
            input_variables=["agent_state_str"],
            partial_variables={"format_instructions": self.output_parser.get_format_instructions()}
        )
        print("âœ… Reflector initialized successfully.")

    def _extract_json_from_response(self, text: str) -> str:
        # (è¿™ä¸ªå‡½æ•°ä¸æˆ‘ä»¬åœ¨å…¶ä»–æ¨¡å—ä¸­ä½¿ç”¨çš„å®Œå…¨ç›¸åŒ)
        match = re.search(r"```json\s*(\{.*?\})\s*```", text, re.DOTALL)
        if match: return match.group(1)
        start_index = text.find('{')
        end_index = text.rfind('}')
        if start_index != -1 and end_index != -1 and end_index > start_index:
            return text[start_index: end_index + 1]
        raise ValueError("Response does not contain a valid JSON object.")

    def _format_agent_state_for_prompt(self, agent_state: AgentState) -> str:
        # åªåŒ…å«ä¸åæ€ç›¸å…³çš„ä¿¡æ¯ï¼Œå‡å°‘tokenæ¶ˆè€—
        step_to_reflect = agent_state.get_step_by_id(agent_state.current_step_id)
        state_for_prompt = {
            "initial_query": agent_state.initial_query,
            "plan": agent_state.plan.model_dump(include={'goal', 'steps'}),
            "current_step_id": agent_state.current_step_id,
            "step_to_reflect": step_to_reflect.model_dump() if step_to_reflect else None
        }
        return json.dumps(state_for_prompt, indent=2, ensure_ascii=False)

    def reflect(self, agent_state: AgentState) -> AgentState:
        """
        å¯¹å½“å‰æ­¥éª¤çš„æ‰§è¡Œç»“æœè¿›è¡Œåæ€ï¼Œå¹¶å°†ç»“æœæ·»åŠ åˆ°AgentStateçš„å†å²è®°å½•ä¸­ã€‚
        """
        print("ğŸ¤” Reflector starting to reflect...")

        # è·å–å½“å‰éœ€è¦åæ€çš„æ­¥éª¤
        step_to_reflect = agent_state.get_step_by_id(agent_state.current_step_id)
        if not step_to_reflect:
            print("âš ï¸ Reflector: No step to reflect upon. Skipping.")
            return agent_state

        agent_state_str = self._format_agent_state_for_prompt(agent_state)

        try:
            # --- ä¸»æµç¨‹ ---
            prompt_value = self.prompt_template.invoke({"agent_state_str": agent_state_str})
            raw_output = self.llm.invoke(prompt_value).content
            json_string = self._extract_json_from_response(raw_output)
            parsed_output: ReflectionOutput = self.output_parser.parse(json_string)

            # ã€æ ¸å¿ƒä¿®æ­£ã€‘å°†LLMçš„è¾“å‡ºä¸æ­¥éª¤çš„å…ƒæ•°æ®åˆå¹¶ï¼Œåˆ›å»ºå®Œæ•´çš„Reflectionå¯¹è±¡
            reflection = Reflection(
                step_id=step_to_reflect.step_id,
                critique=parsed_output.critique,
                is_success=parsed_output.is_success,
                confidence=parsed_output.confidence,
                suggestion=parsed_output.suggestion,
                is_finished=parsed_output.is_finished  # ä»LLMçš„è¾“å‡ºä¸­è·å–
            )
            print(f"âœ… Reflection successful: {reflection.critique}")

        except Exception as e:
            # --- å¤‡ç”¨æ–¹æ¡ˆ ---
            print(f"âŒ Reflector failed to get a valid reflection from LLM: {e}")
            # ã€æ ¸å¿ƒä¿®æ­£ã€‘åˆ›å»ºå¤‡ç”¨Reflectionå¯¹è±¡æ—¶ï¼Œæä¾›æ‰€æœ‰å¿…éœ€çš„å­—æ®µ
            reflection = Reflection(
                step_id=step_to_reflect.step_id,
                critique="åæ€æ¨¡å—åœ¨å°è¯•è§£æLLMè¾“å‡ºæ—¶é‡åˆ°å†…éƒ¨é”™è¯¯ã€‚",
                is_success=step_to_reflect.is_success,  # ç›´æ¥æ²¿ç”¨ä¸Šä¸€æ­¥çš„æˆåŠŸçŠ¶æ€
                confidence=0.0,  # ç½®ä¿¡åº¦ä¸º0
                suggestion="å»ºè®®é‡æ–°è§„åˆ’ä»¥å°è¯•ä¸åŒçš„æ–¹æ³•ã€‚",
                is_finished=False  # æ— æ³•åˆ¤æ–­æ˜¯å¦å®Œæˆï¼Œé»˜è®¤ä¸ºFalse
            )

        # å°†æ–°ç”Ÿæˆçš„åæ€å¯¹è±¡æ·»åŠ åˆ°å†å²è®°å½•ä¸­
        agent_state.history.append(reflection)
        return agent_state