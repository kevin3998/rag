import streamlit as st
import sys
from pathlib import Path

# --- é¡¹ç›®è·¯å¾„è®¾ç½® ---
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# --- å¯¼å…¥æ ¸å¿ƒç»„ä»¶ ---
from rag_system.agent.agent_executor import MaterialScienceAgent
from rag_system.config import settings

# --- é¡µé¢é…ç½®ä¸ç»„ä»¶åŠ è½½ ---
st.set_page_config(page_title="ææ–™ç§‘å­¦AIåŠ©æ‰‹", page_icon="ğŸ§ª", layout="wide")


@st.cache_resource
def load_agent():
    """
    åŠ è½½å¹¶ç¼“å­˜æ™ºèƒ½ä½“å®ä¾‹ã€‚
    ã€å…³é”®ä¿®æ”¹ã€‘ç§»é™¤äº†é”™è¯¯çš„ st.set_option è°ƒç”¨ã€‚
    """
    return MaterialScienceAgent()


st.title("ğŸ§ª ææ–™ç§‘å­¦AIåŠ©æ‰‹")
st.caption(f"ç”±æœ¬åœ°æ¨¡å‹ '{settings.LOCAL_LLM_MODEL_NAME}' é©±åŠ¨ (ReActæ¶æ„)")

agent = load_agent()

if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("å…³äºç³»ç»Ÿ")
    st.info("è¿™æ˜¯ä¸€ä¸ªåŸºäºReActæ¡†æ¶çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè°ƒç”¨å·¥å…·æ¥å›ç­”ä¸“ä¸šé—®é¢˜ã€‚")
    if st.button("æ¸…é™¤èŠå¤©è®°å½•", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- ä¸»èŠå¤©ç•Œé¢ ---
# 1. æ¸²æŸ“æ‰€æœ‰å†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] == "assistant" and message.get("think_process"):
            with st.expander("æŸ¥çœ‹AIæ€è€ƒè¿‡ç¨‹", expanded=False):
                st.markdown(message["think_process"], unsafe_allow_html=True)
        st.markdown(message["content"])

# 2. æ¥æ”¶å¹¶å¤„ç†æ–°è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # ä¸ºå®æ—¶æ¸²æŸ“å‡†å¤‡UIå ä½ç¬¦
        answer_placeholder = st.empty()
        with st.expander("AIæ€è€ƒè¿‡ç¨‹", expanded=True) as think_process_expander:
            log_placeholder = st.empty()

        full_response = ""
        think_process_log = ""

        try:
            stream = agent.run(prompt)
            for chunk in stream:
                if "log" in chunk:
                    think_process_log += chunk["log"].strip().replace('<', '&lt;').replace('>', '&gt;') + "\n\n"
                    log_placeholder.markdown(think_process_log)
                elif "output" in chunk:
                    full_response += chunk["output"]
                    answer_placeholder.markdown(full_response + "â–Œ")
        except Exception as e:
            st.error(f"å¤„ç†æ—¶å‡ºç°é”™è¯¯: {e}", icon="ğŸš¨")
            full_response = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ã€‚"

        # æµç¨‹ç»“æŸåï¼Œæœ€ç»ˆæ›´æ–°UI
        answer_placeholder.markdown(full_response)
        # æ£€æŸ¥æ€è€ƒæ—¥å¿—æ˜¯å¦å­˜åœ¨ï¼Œå†æ›´æ–°å…¶æœ€ç»ˆçŠ¶æ€
        if think_process_log:
            log_placeholder.markdown(think_process_log)

    # å°†æœ¬æ¬¡äº¤äº’çš„å®Œæ•´ç»“æœå­˜å…¥å†å²è®°å½•
    st.session_state.messages.append({
        "role": "assistant",
        "content": full_response,
        "think_process": think_process_log
    })
    # ä½¿ç”¨rerunæ¥ç¡®ä¿UIçŠ¶æ€æ­£ç¡®åˆ·æ–°ï¼Œå¹¶å°†æ–°æ¶ˆæ¯å˜ä¸ºå†å²
    st.rerun()